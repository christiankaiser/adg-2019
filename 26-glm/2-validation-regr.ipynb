{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation d'un modèle de régression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lorsque nous avons une modèle de régression linéaire simple, l'évaluation de la qualité du modèle est relativement simple. Cependant, plus un modèle de régression est sophistiqué, plus il se pose la question de comment valider le modèle.\n",
    "\n",
    "Une régression est au final un modèle de prédiction ou d'estimation pour la variable dépendante sur la base des variables indépendantes. La meilleure façon d'évaluer la qualité du modèle est d'**évaluer sa capacité de faire des prédictions correctes sur la base de nouvelles données**. Ceci permet également d'éviter qu'un modèle puisse se baser sur le fait d'avoir déjà «vu» une donnée particulière et de l'utiliser pour la prédiction. Ceci porte le danger de faire une prédiction parfaite pour toutes les données connues, et de faire une mauvaise prédiction dans tous les autres cas. On dit alors que le modèle fait du **over-fitting** et qu'il **généralise mal** aux nouvelles données. C'est un problème avec tous les modèles de régression sophistiqués.\n",
    "\n",
    "La stratégie générale que nous pouvons adapter est la suivante: le jeu de données que nous avons au début est divisé en deux jeux:\n",
    "\n",
    "1. un **jeu de données de calibration** pour déterminer les paramètres du modèle\n",
    "2. un **jeu de données de test** pour déterminer la qualité du modèle au niveau de la capacité de prédiction\n",
    "\n",
    "Nous allons typiquement utiliser 80-90% du jeu de données initial pour la calibration du modèle, et le 10-20% restant pour la validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons comment on peut diviser un jeu de données initial en jeux de données de calibration et de test. Commençons par lire un fichier de données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = read.csv(file=\"ch-socioeco-typologie.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cas, nous utilisons 90% des données pour le jeu de données de calibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = sample(nrow(d), size=0.9*nrow(d))\n",
    "dtrain = d[idx,]\n",
    "dtest = d[-idx,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les 90% sont spécifiés avec le `0.9` dans le paramètre `size`. Il faut évidemment encore ajuster le nom de la variable `d`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir illustrer l'évaluation d'un modèle, nous devons d'abord en faire un. Prenons le [modèle de régression logistique de la semaine passée](../25-regression/regression-logistique.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reglogit = glm(typologie_binaire ~ ADO + NADHO + NADRET + AD3PRIM + AD3SEC, family=binomial(link=\"logit\"), data=dtrain) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que nous avons utilisé le jeu de données __`dtrain`__ et non `d`. \n",
    "\n",
    "Maintenant nous pouvons appliquer le modèle au jeu de données de test (`newdata=dtest`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reglogit_predictions = predict(reglogit, newdata=dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas d'une régression logistique, nous pouvons faire un **tableau simple avec le nombre de cas prédits correctement ou non**.\n",
    "\n",
    "Avec notre régression logistique binomiale, nous devons d'abord traduire la probabilité estimée en classes (`U` pour les centres urbains, `N` pour les autres), ce que nous pouvons faire avec une condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_classes = ifelse(reglogit_predictions > 0.5, 'U', 'N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et maintenant nous pouvons dresser notre tableau de comparaison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            \n",
       "pred_classes  N  U\n",
       "           N 40 13\n",
       "           U 13 71"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(pred_classes, dtest$typologie_binaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que ce tableau est très similaire que nous avons déjà calculé la semaine passée, sauf que maintenant nous l'avons calculé sur le jeu de données de test.\n",
    "\n",
    "Et nous pouvons calculer la proportion de réponses justes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.81021897810219"
      ],
      "text/latex": [
       "0.81021897810219"
      ],
      "text/markdown": [
       "0.81021897810219"
      ],
      "text/plain": [
       "[1] 0.810219"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(pred_classes == dtest$typologie_binaire) / nrow(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ce qui nous donne 81% de réponses justes pour le jeu de données de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons évidemment faire le même calcul pour le jeu de données de calibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.85586319218241"
      ],
      "text/latex": [
       "0.85586319218241"
      ],
      "text/markdown": [
       "0.85586319218241"
      ],
      "text/plain": [
       "[1] 0.8558632"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_train = predict(reglogit, newdata=dtrain)\n",
    "pred_train_classes = ifelse(pred_train > 0.5, 'U', 'N')\n",
    "sum(pred_train_classes == dtrain$typologie_binaire) / nrow(dtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la grande majorité des cas, la proportion de réponses justes est plus élevée pour le jeu de données de calibration. Ceci est normal étant donné que le modèle a pu être ajusté spécifiquement à ce jeu de données de calibrage. Par contre, c'est uniquement la proportion de réponses justes pour le jeu de données de test qui est véritablement importante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les cas où la **variable dépendante est continue** (et non catégorielle), nous pouvons par exemple calculer l'erreur absolu moyen (Mean Absolute Error MAE) sur la base du jeu de données de test. Voici l'exemple de la régression linéaire de la semaine passée:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d2 = read.csv(file=\"../25-regression/data-zh-be.tsv\", sep=\"\\t\")\n",
    "idx = sample(nrow(d2), size=0.9*nrow(d2))\n",
    "dtrain2 = d2[idx,]\n",
    "dtest2 = d2[-idx,]\n",
    "reglin = lm(PMSDIV ~ ADCFARM + PRPROT + PRCATH + PRJEW + PFGEN + PFBAC, data=dtrain2)\n",
    "lm_pred = predict(reglin, newdata=dtest2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lm_pred` contient maintenant les valeurs prédites pour le jeu de données test. Nous pouvons calculer l'erreur absolu moyen avec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "  6.046  26.430  43.800  63.760  75.760 239.100 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(abs(lm_pred - dtest2$PMSDIV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons donc un erreur moyen d'environ 64 personnes pour le nombre de personnes divorcées. L'erreur absolu médian quant à lui est d'environ 44 personnes.\n",
    "\n",
    "Est-ce que c'est beaucoup ou peu? Nous pouvons mettre en relation cette valeur avec le nombre de personnes divorcées:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "erreur_relative = abs(lm_pred - dtest2$PMSDIV) / dtest2$PMSDIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n",
       "0.01356 0.05250 0.09845 0.13540 0.17640 0.72430 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(erreur_relative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ce qui nous fait donc un erreur absolu moyen relatif d'environ 13.5%, et l'erreur absolu médian relatif d'environ 9.8%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation (validation croisée)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode de validation présentée ci-dessus est très simple et facile à comprendre. Par contre, elle présente quelques problèmes:\n",
    "\n",
    "- en cas de petits jeux de données, réserver une bonne partie des données uniquement pour les tests peut être problématique\n",
    "\n",
    "- en raison de la sélection aléatoire du jeu de données de calibration et de test, chaque calcul du modèle donnera une erreur légèrement différente\n",
    "\n",
    "En conséquence, plusieurs méthodes alternatives ont été développées. La cross-validation est une telle alternative, qui est plutôt une famille de méthodes que d'une seule méthode, car il y a beaucoup de variantes qui existent. Nous allons nous limiter à deux cas, la ___«leave-one-out cross-validation»___ et la ___«k-fold cross-validation»___.\n",
    "\n",
    "Dans le cas de la **«leave-one-out cross-validation»**, on enlève une ligne du jeu de données, puis on calcule le modèle sur les données restante, et on calcule l'erreur d'estimation pour la ligne enlevée. On refait cette procédure pour chaque ligne, et on calcule l'erreur moyen.\n",
    "\n",
    "Dans le cas de la ___«k-fold cross-validation»___, on divise le jeu de données initial en $k$ partitions de taille égale. Une partition sert de jeu de validation et les autres $k-1$ partitions pour la calibration. Puis on refait la même procédure avec une autre partition pour la validation.\n",
    "\n",
    "Le principe reste au final le même que celui présenté plus haut.\n",
    "\n",
    "Nous n'allons pas calculer la validation croisée ici, mais il est une bonne chose de connaître à peu près le principe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
